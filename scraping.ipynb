{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from threading import Thread\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cliente = MongoClient('localhost', 27017)\n",
    "db = cliente['noticias']\n",
    "coleccion = db['noticia']\n",
    "enlaces = []\n",
    "enlaces_aux = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(soup):\n",
    "    for link in soup.find_all(\"a\"):\n",
    "        enlace = link.get(\"href\")\n",
    "        if enlace and enlace.startswith(\"https://www.xataka.com.mx/\") and enlace not in enlaces and enlace not in enlaces_aux:\n",
    "            enlaces_aux.append(enlace)\n",
    "            enlaces.append(enlace)\n",
    "    return enlaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title(soup):\n",
    "    try:\n",
    "        title = soup.find('h1')\n",
    "        return title.text\n",
    "    except:\n",
    "        return \"No hay titulo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_content(soup):\n",
    "    try:\n",
    "        articulo = soup.find(\"div\", attrs={\"class\", \"article-content\"})\n",
    "        text = articulo.get_text()\n",
    "        words = text.split(\".\")\n",
    "        text = words[0] + \".\" + words[1] + \".\"\n",
    "        return text\n",
    "    except:\n",
    "        return 'Error en extracion de articulo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_and_ranking(soup):\n",
    "    try:\n",
    "        articulo = soup.find(\"div\", attrs={\"class\", \"article-content\"})\n",
    "        texto = articulo.get_text()\n",
    "        texto = texto.lower()\n",
    "        #Obtener las stopwords en spanish\n",
    "        texto = re.sub(r'[^\\w\\s]', '', texto)\n",
    "        stopwords_es = set(stopwords.words())\n",
    "        # Tokenizar el texto en palabras\n",
    "        palabras = word_tokenize(texto)\n",
    "        \n",
    "        # Filtrar las stopwords\n",
    "        palabras_sin_stopwords = [palabra for palabra in palabras if not palabra in stopwords_es]\n",
    "\n",
    "        # Unir las palabras filtradas en un nuevo texto\n",
    "        texto_sin_stopwords = ' '.join(palabras_sin_stopwords)\n",
    "        texto_sin_stopwords\n",
    "        \n",
    "        palabras_repetidas = texto_sin_stopwords.split()\n",
    "        contador = Counter(palabras_repetidas)\n",
    "        comunes = contador.most_common(3)\n",
    "        return comunes\n",
    "    except:\n",
    "        num_rows = 3  # número de filas\n",
    "        num_cols = 3  # número de columnas\n",
    "        error_matrix = [[\"Error\" for j in range(num_cols)] for i in range(num_rows)]\n",
    "        return error_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_mongodb(titulo, articulo, comunes, url, revisado=False):\n",
    "    \"\"\"Inserta en mongo la informacion encontrada en las distintas funciones\n",
    "\n",
    "    Args:\n",
    "        titulo (string): titulo de la noticia\n",
    "        articulo (string): descripcion principal de que trata la noticia\n",
    "        comunes (lista): arreglo de las palabras mas comunes y cuantas veces se repiten\n",
    "        url (string): url de la noticia a guardar\n",
    "        revisado (bool, optional): verificacion si el enlace ya fue escaneado o no. Defaults to False.\n",
    "    \"\"\"\n",
    "\n",
    "    registro = {\n",
    "        \"titulo\": titulo.strip(), \"articulo\": articulo.strip(), \n",
    "        \"revisado\": revisado,\n",
    "        \"palabra 1\":comunes[0][0], \"rank 1\": comunes[0][1],\n",
    "        \"palabra 2\":comunes[1][0], \"rank 2\": comunes[1][1],\n",
    "        \"palabra 3\":comunes[2][0], \"rank 3\": comunes[2][1],\n",
    "        \"url\": url.strip()\n",
    "        }\n",
    "    if articulo != \"Error en extracion de articulo\":\n",
    "        operacion = coleccion.insert_one(registro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def primer_hilo(url):\n",
    "    \"\"\"Hilo de busqueda de informacion\n",
    "\n",
    "    Args:\n",
    "        url (string): url de la noticia a scanear\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    enlaces = get_links(soup)\n",
    "    titulo = get_title(soup).strip()\n",
    "    contenido = get_article_content(soup).strip()\n",
    "    palabras_rankeadas = get_word_and_ranking(soup)\n",
    "    insert_mongodb(titulo, contenido, palabras_rankeadas, url, revisado=True)\n",
    "    #print(f'Enlaces encontrados: {len(enlaces)}')\n",
    "    #print(f'Titulo: {titulo}')\n",
    "    #print(f'Contenido: \\n {contenido}')\n",
    "    #print(f'Palabras mas comunes: {palabras_rankeadas}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segundo_hilo(url):\n",
    "    \"\"\"Hilo de busqueda de informacion\n",
    "\n",
    "    Args:\n",
    "        url (string): url de la noticia a scanear\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    enlaces = get_links(soup)\n",
    "    titulo = get_title(soup).strip()\n",
    "    contenido = get_article_content(soup).strip()\n",
    "    palabras_rankeadas = get_word_and_ranking(soup)\n",
    "    insert_mongodb(titulo, contenido, palabras_rankeadas, url, revisado=True)\n",
    "    #print(f'Enlaces encontrados: {len(enlaces)}')\n",
    "    #print(f'Titulo: {titulo}')\n",
    "    #print(f'Contenido: \\n {contenido}')\n",
    "    #print(f'Palabras mas comunes: {palabras_rankeadas}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tercer_hilo(url):\n",
    "    \"\"\"Hilo de busqueda de informacion\n",
    "\n",
    "    Args:\n",
    "        url (string): url de la noticia a scanear\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    enlaces = get_links(soup)\n",
    "    titulo = get_title(soup).strip()\n",
    "    contenido = get_article_content(soup).strip()\n",
    "    palabras_rankeadas = get_word_and_ranking(soup)\n",
    "    insert_mongodb(titulo, contenido, palabras_rankeadas, url, revisado=True)\n",
    "    #print(f'Enlaces encontrados: {len(enlaces)}')\n",
    "    #print(f'Titulo: {titulo}')\n",
    "    #print(f'Contenido: \\n {contenido}')\n",
    "    #print(f'Palabras mas comunes: {palabras_rankeadas}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cuarto_hilo(url):\n",
    "    \"\"\"Hilo de busqueda de informacion\n",
    "\n",
    "    Args:\n",
    "        url (string): url de la noticia a scanear\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    enlaces = get_links(soup)\n",
    "    titulo = get_title(soup).strip()\n",
    "    contenido = get_article_content(soup).strip()\n",
    "    palabras_rankeadas = get_word_and_ranking(soup)\n",
    "    insert_mongodb(titulo, contenido, palabras_rankeadas, url, revisado=True)\n",
    "    #print(f'Enlaces encontrados: {len(enlaces)}')\n",
    "    #print(f'Titulo: {titulo}')\n",
    "    #print(f'Contenido: \\n {contenido}')\n",
    "    #print(f'Palabras mas comunes: {palabras_rankeadas}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quinto_hilo(url):\n",
    "    \"\"\"Hilo de busqueda de informacion\n",
    "\n",
    "    Args:\n",
    "        url (string): url de la noticia a scanear\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    enlaces = get_links(soup)\n",
    "    titulo = get_title(soup).strip()\n",
    "    contenido = get_article_content(soup).strip()\n",
    "    palabras_rankeadas = get_word_and_ranking(soup)\n",
    "    insert_mongodb(titulo, contenido, palabras_rankeadas, url, revisado=True)\n",
    "    #print(f'Enlaces encontrados: {len(enlaces)}')\n",
    "    #print(f'Titulo: {titulo}')\n",
    "    #print(f'Contenido: \\n {contenido}')\n",
    "    #print(f'Palabras mas comunes: {palabras_rankeadas}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sexto_hilo(url):\n",
    "    \"\"\"Hilo de busqueda de informacion\n",
    "\n",
    "    Args:\n",
    "        url (string): url de la noticia a scanear\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    enlaces = get_links(soup)\n",
    "    titulo = get_title(soup).strip()\n",
    "    contenido = get_article_content(soup).strip()\n",
    "    palabras_rankeadas = get_word_and_ranking(soup)\n",
    "    insert_mongodb(titulo, contenido, palabras_rankeadas, url, revisado=True)\n",
    "    #print(f'Enlaces encontrados: {len(enlaces)}')\n",
    "    #print(f'Titulo: {titulo}')\n",
    "    #print(f'Contenido: \\n {contenido}')\n",
    "    #print(f'Palabras mas comunes: {palabras_rankeadas}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def septimo_hilo(url):\n",
    "    \"\"\"Hilo de busqueda de informacion\n",
    "\n",
    "    Args:\n",
    "        url (string): url de la noticia a scanear\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    enlaces = get_links(soup)\n",
    "    titulo = get_title(soup).strip()\n",
    "    contenido = get_article_content(soup).strip()\n",
    "    palabras_rankeadas = get_word_and_ranking(soup)\n",
    "    insert_mongodb(titulo, contenido, palabras_rankeadas, url, revisado=True)\n",
    "    #print(f'Enlaces encontrados: {len(enlaces)}')\n",
    "    #print(f'Titulo: {titulo}')\n",
    "    #print(f'Contenido: \\n {contenido}')\n",
    "    #print(f'Palabras mas comunes: {palabras_rankeadas}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def octavo_hilo(url):\n",
    "    \"\"\"Hilo de busqueda de informacion\n",
    "\n",
    "    Args:\n",
    "        url (string): url de la noticia a scanear\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    enlaces = get_links(soup)\n",
    "    titulo = get_title(soup).strip()\n",
    "    contenido = get_article_content(soup).strip()\n",
    "    palabras_rankeadas = get_word_and_ranking(soup)\n",
    "    insert_mongodb(titulo, contenido, palabras_rankeadas, url, revisado=True)\n",
    "    #print(f'Enlaces encontrados: {len(enlaces)}')\n",
    "    #print(f'Titulo: {titulo}')\n",
    "    #print(f'Contenido: \\n {contenido}')\n",
    "    #print(f'Palabras mas comunes: {palabras_rankeadas}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enlaces encontrados: 78\n",
      "Titulo: Xataka México\n",
      "Contenido: \n",
      " Error en extracion de articulo\n",
      "Palabras mas comunes: [['Error', 'Error', 'Error'], ['Error', 'Error', 'Error'], ['Error', 'Error', 'Error']]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m hilo_6 \u001b[39m=\u001b[39m Thread(target\u001b[39m=\u001b[39msexto_hilo(enlace_6))\n\u001b[0;32m     34\u001b[0m enlace_7 \u001b[39m=\u001b[39m enlaces\u001b[39m.\u001b[39mpop()\n\u001b[1;32m---> 35\u001b[0m hilo_7 \u001b[39m=\u001b[39m Thread(target\u001b[39m=\u001b[39mseptimo_hilo(enlace_7))\n\u001b[0;32m     37\u001b[0m enlace_8 \u001b[39m=\u001b[39m enlaces\u001b[39m.\u001b[39mpop()\n\u001b[0;32m     38\u001b[0m hilo_8 \u001b[39m=\u001b[39m Thread(target\u001b[39m=\u001b[39moctavo_hilo(enlace_8))\n",
      "Cell \u001b[1;32mIn[14], line 7\u001b[0m, in \u001b[0;36mseptimo_hilo\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mseptimo_hilo\u001b[39m(url):\n\u001b[0;32m      2\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Hilo de busqueda de informacion\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39m        url (string): url de la noticia a scanear\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m     response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mget(url)\n\u001b[0;32m      8\u001b[0m     soup \u001b[39m=\u001b[39m BeautifulSoup(response\u001b[39m.\u001b[39mtext, \u001b[39m'\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m     enlaces \u001b[39m=\u001b[39m get_links(soup)\n",
      "File \u001b[1;32mc:\\Users\\luian\\DevSpace\\Scrapping-Startups\\env\\Lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39;49m\u001b[39mget\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, params\u001b[39m=\u001b[39;49mparams, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\luian\\DevSpace\\Scrapping-Startups\\env\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\luian\\DevSpace\\Scrapping-Startups\\env\\Lib\\site-packages\\requests\\sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    582\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m    583\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[0;32m    584\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    585\u001b[0m }\n\u001b[0;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[0;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\luian\\DevSpace\\Scrapping-Startups\\env\\Lib\\site-packages\\requests\\sessions.py:723\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m \u001b[39mif\u001b[39;00m allow_redirects:\n\u001b[0;32m    721\u001b[0m     \u001b[39m# Redirect resolving generator.\u001b[39;00m\n\u001b[0;32m    722\u001b[0m     gen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresolve_redirects(r, request, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 723\u001b[0m     history \u001b[39m=\u001b[39m [resp \u001b[39mfor\u001b[39;49;00m resp \u001b[39min\u001b[39;49;00m gen]\n\u001b[0;32m    724\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    725\u001b[0m     history \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\luian\\DevSpace\\Scrapping-Startups\\env\\Lib\\site-packages\\requests\\sessions.py:723\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    720\u001b[0m \u001b[39mif\u001b[39;00m allow_redirects:\n\u001b[0;32m    721\u001b[0m     \u001b[39m# Redirect resolving generator.\u001b[39;00m\n\u001b[0;32m    722\u001b[0m     gen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresolve_redirects(r, request, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 723\u001b[0m     history \u001b[39m=\u001b[39m [resp \u001b[39mfor\u001b[39;00m resp \u001b[39min\u001b[39;00m gen]\n\u001b[0;32m    724\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    725\u001b[0m     history \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\luian\\DevSpace\\Scrapping-Startups\\env\\Lib\\site-packages\\requests\\sessions.py:266\u001b[0m, in \u001b[0;36mSessionRedirectMixin.resolve_redirects\u001b[1;34m(self, resp, req, stream, timeout, verify, cert, proxies, yield_requests, **adapter_kwargs)\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[39myield\u001b[39;00m req\n\u001b[0;32m    264\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 266\u001b[0m     resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(\n\u001b[0;32m    267\u001b[0m         req,\n\u001b[0;32m    268\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    269\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    270\u001b[0m         verify\u001b[39m=\u001b[39;49mverify,\n\u001b[0;32m    271\u001b[0m         cert\u001b[39m=\u001b[39;49mcert,\n\u001b[0;32m    272\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m    273\u001b[0m         allow_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    274\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49madapter_kwargs,\n\u001b[0;32m    275\u001b[0m     )\n\u001b[0;32m    277\u001b[0m     extract_cookies_to_jar(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcookies, prepared_request, resp\u001b[39m.\u001b[39mraw)\n\u001b[0;32m    279\u001b[0m     \u001b[39m# extract redirect url, if any, for the next loop\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\luian\\DevSpace\\Scrapping-Startups\\env\\Lib\\site-packages\\requests\\sessions.py:745\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[0;32m    744\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m stream:\n\u001b[1;32m--> 745\u001b[0m     r\u001b[39m.\u001b[39;49mcontent\n\u001b[0;32m    747\u001b[0m \u001b[39mreturn\u001b[39;00m r\n",
      "File \u001b[1;32mc:\\Users\\luian\\DevSpace\\Scrapping-Startups\\env\\Lib\\site-packages\\requests\\models.py:899\u001b[0m, in \u001b[0;36mResponse.content\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_content \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    898\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 899\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_content \u001b[39m=\u001b[39m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39miter_content(CONTENT_CHUNK_SIZE)) \u001b[39mor\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    901\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_content_consumed \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    902\u001b[0m \u001b[39m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[0;32m    903\u001b[0m \u001b[39m# since we exhausted the data.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\luian\\DevSpace\\Scrapping-Startups\\env\\Lib\\site-packages\\requests\\models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    815\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 816\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    817\u001b[0m     \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    818\u001b[0m         \u001b[39mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32mc:\\Users\\luian\\DevSpace\\Scrapping-Startups\\env\\Lib\\site-packages\\urllib3\\response.py:624\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    608\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    609\u001b[0m \u001b[39mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[0;32m    610\u001b[0m \u001b[39m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    621\u001b[0m \u001b[39m    'content-encoding' header.\u001b[39;00m\n\u001b[0;32m    622\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    623\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunked \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msupports_chunked_reads():\n\u001b[1;32m--> 624\u001b[0m     \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mread_chunked(amt, decode_content\u001b[39m=\u001b[39mdecode_content):\n\u001b[0;32m    625\u001b[0m         \u001b[39myield\u001b[39;00m line\n\u001b[0;32m    626\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\luian\\DevSpace\\Scrapping-Startups\\env\\Lib\\site-packages\\urllib3\\response.py:828\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    825\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    827\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 828\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_chunk_length()\n\u001b[0;32m    829\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunk_left \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    830\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\luian\\DevSpace\\Scrapping-Startups\\env\\Lib\\site-packages\\urllib3\\response.py:758\u001b[0m, in \u001b[0;36mHTTPResponse._update_chunk_length\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    756\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunk_left \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    757\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m--> 758\u001b[0m line \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mfp\u001b[39m.\u001b[39mreadline()\n\u001b[0;32m    759\u001b[0m line \u001b[39m=\u001b[39m line\u001b[39m.\u001b[39msplit(\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m;\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m1\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m    760\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.752.0_x64__qbz5n2kfra8p0\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    707\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    708\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.752.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1278\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1274\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1275\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1276\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1277\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1278\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1279\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1280\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.752.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1134\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1132\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1133\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1134\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1135\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1136\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "url_base = 'https://www.xataka.com.mx/'\n",
    "request = requests.get(url_base)\n",
    "soup = BeautifulSoup(request.text)\n",
    "enlaces = get_links(soup)\n",
    "titulo = get_title(soup).strip()\n",
    "contenido = get_article_content(soup).strip()\n",
    "palabras_rankeadas = get_word_and_ranking(soup)\n",
    "print(f'Enlaces encontrados: {len(enlaces)}')\n",
    "print(f'Titulo: {titulo}')\n",
    "print(f'Contenido: \\n {contenido}')\n",
    "print(f'Palabras mas comunes: {palabras_rankeadas}')\n",
    "insert_mongodb(titulo, contenido, palabras_rankeadas, url_base, revisado=True)\n",
    "contador = 0\n",
    "try:\n",
    "    for enlace in enlaces:\n",
    "        enlace_1 = enlaces.pop()\n",
    "        hilo_1 = Thread(target=primer_hilo(enlace_1))\n",
    "\n",
    "        enlace_2 = enlaces.pop()\n",
    "        hilo_2 = Thread(target=segundo_hilo(enlace_2))\n",
    "\n",
    "        enlace_3 = enlaces.pop()\n",
    "        hilo_3 = Thread(target=tercer_hilo(enlace_3))\n",
    "\n",
    "        enlace_4 = enlaces.pop()\n",
    "        hilo_4 = Thread(target=cuarto_hilo(enlace_4))\n",
    "\n",
    "        enlace_5 = enlaces.pop()\n",
    "        hilo_5 = Thread(target=quinto_hilo(enlace_5))\n",
    "        \n",
    "        enlace_6 = enlaces.pop()\n",
    "        hilo_6 = Thread(target=sexto_hilo(enlace_6))\n",
    "\n",
    "        enlace_7 = enlaces.pop()\n",
    "        hilo_7 = Thread(target=septimo_hilo(enlace_7))\n",
    "        \n",
    "        enlace_8 = enlaces.pop()\n",
    "        hilo_8 = Thread(target=octavo_hilo(enlace_8))\n",
    "\n",
    "        contador += 1\n",
    "except Exception as e:\n",
    "    print(\"Error en la ejecucion del hilo\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de veces que ejecuta los hilos: 200\n",
      "Enlaces revisados: 1937\n",
      "Enlaces encontrados: 3544\n",
      "Enlaces insertados que no contenian ningun error: 1495\n"
     ]
    }
   ],
   "source": [
    "print(f'Numero de veces que ejecuta los hilos: {contador}')\n",
    "print(f'Enlaces revisados: {len(enlaces)}')\n",
    "print(f'Enlaces encontrados: {len(enlaces_aux)}')\n",
    "\n",
    "noticias_bd = []\n",
    "for noticiax in coleccion.find():\n",
    "    noticias_bd.append(noticiax[\"url\"])\n",
    "print(f'Enlaces insertados que no contenian ningun error: {len(noticias_bd)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "04df42d258e3e6c1bd4a4e9ee27d3cee905bb2838ef9543630bc10527098a375"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
